{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to flauers\n",
    "\n",
    "Flauers (IPA: ËˆflaÊŠÉ™rs', same pronunciation as \"flowers\") is a tool that allows the simulation of matrix-multiplication systolic arrays with seamless integration in pytorch for neural network evaluation. \n",
    "\n",
    "This tutorial will give you the basics of simulated fault injection using flauers.\n",
    "\n",
    "First of all, we want to import the packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flauers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! It seems like we cannot import the package! Something is missing... We forgot to install it!\n",
    "Well, as of now, flauers does not provide any pip or conda package ready to be installed. The easyest thing is to add the path to your current python envirnoment. So, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\") # you always want to `append` the src path this repo\n",
    "import flauers\n",
    "# we print it just to verify it is working correctly.\n",
    "print(flauers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Seems like everything went smoothly. \n",
    "We can try some matrix multiplication now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # We need it to intantiate the matrices ðŸ¤­\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "B = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(\"A is\\n\",A, end=\"\\n\\n\")\n",
    "print(\"B is\\n\",B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our matrices, let's try multipliyng them with embedded python BLAS routines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C = np.matmul(A, B) # equivalent to A@B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try multiplying it with flauers now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flauers.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, right?!\n",
    "\n",
    "So, now that we know how to do matrix multiplication let's go for some details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "The goal of flauers is to simulate some hardware at logic level.\n",
    "Real hardware has buses and operators of fixed size. flauers models this aspect of the hardware through datatypes (also called dtypes).\n",
    "\n",
    "As of now, only a subset of numpy dtypes are supported: int8, int16, int32, int64, float32, float64.\n",
    "Nevertheless they should be enough for neural networks\n",
    "\n",
    "When generating a `SystolicArray` object (more about that later) implicitly it is generated with int8 buses for the inputs.\n",
    "Also take into account the fact that there are two main dtype: input dtypes and accumulation dtypes.\n",
    "\n",
    "Anyways, let's try some floating points (more common in NNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_float = A.astype(np.float32)\n",
    "B_float = B.astype(np.float32)\n",
    "A_float # just to display it in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the matrices are now float32. \n",
    "\n",
    "Let's try as we did again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flauers.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work! How comes?\n",
    "\n",
    "The answer is simple: even if A_float and B_float have a different dtype, it was possible to cast all the values to int8, and this conversion was performed implicitly. \n",
    "\n",
    "Let's try multiplying the matrices for 0.5 and do the multiplication again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_float = A * 0.5\n",
    "A_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_float = B*0.5\n",
    "B_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_float @ B_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flauers.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an error was raised. A CastingError error in facts! Pretty self explanatory!\n",
    "On top of that, additional informations are displayed, but we don't care about those.\n",
    "\n",
    "We immediately see the problem: it says that it \"couldnt convert A from float64 to int8\". Perfect! \n",
    "\n",
    "So, what do we do? We can explicitly create a `SystolicArray` object with appropriate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw = flauers.SystolicArray(10, 10, 10, flauers.projection_matrices.output_stationary, in_dtype=np.float32)\n",
    "hw.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this worked like a charm! ðŸ˜Ž\n",
    "\n",
    "There are some parameters that are \"weird\": what are all of those 10 and what is a projection matrix? \n",
    "There's quite a lot happening behind the scenes, so let's not care too much about everything. Refer to the documentation for details about that.\n",
    "\n",
    "Nevertheless, here a quick explaination:\n",
    "\n",
    "- The first three parameters (namely $N1, N2, N3$) define the size (in space and time) of the systolic array. Specifically, if we have matrices A (of dimensions $X, Y$) and B (of dimension $Y, Z$), we need to ensure that $N1 >= X$, $N3 >= Z$ and $N2 >= Y$. The reason behind it is due to the algorithm used to simulate the `SystolicArray`\n",
    "- The projection matrice determines the shape of the systolic array. Specifically implying which registers are reused and how. \n",
    "\n",
    "The following figure shows the difference among three different systolic array. Even though they have the same $N1, N2, N3$ parameters, the projection matrix changes the shape and the way the data is forwarded.\n",
    "\n",
    "In any case, you can find more details in the documentation and in [this paper](http://compalg.inf.elte.hu/~tony/Informatikai-Konyvtar/03-Algorithms%20of%20Informatics%201,%202,%203/Systolic30May.pdf)\n",
    "\n",
    "> Please note that we always perform the computation $R = A \\times B + C$ using $A$ as the activations, $B$ as the weights and $C$ as the _accumulation_ (which is both the added term $C$ and the partial sum of the matrix multiplication).\n",
    "> Later on you will see that each processing element has three registers a, b and c which directly correspond to the values of matrices $A, B, C$.\n",
    "\n",
    "> In the figures you can also see how a fault in a register affects the output matrix, depending on the injected register and the projection matrix\n",
    "\n",
    "### No local reuse:\n",
    "![nlr](misc/NoLocalReuse.drawio.png)\n",
    "\n",
    "### Output Stationary:\n",
    "![os](misc/OutputStationary.drawio.png)\n",
    "\n",
    "### Row Stationary:\n",
    "![rs](misc/RowStationary.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Injection\n",
    "\n",
    "The whole point of flauers is to simulate Systolic Arrays and being able to perform fault injection on it. \n",
    "So, how do we perform fault injection? We simply create a `Fault` and we add it to the hardware. \n",
    "\n",
    "But first, we have to point out that the _processing elements_ (PEs) of the systolic array have all the same structure, which is the one in the following picture.\n",
    "\n",
    "![basic_processing_element](misc/basic_processing_element.drawio.png)\n",
    "\n",
    "When we create a fault, we have to choose which register to inject among `a`, `b` and `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want a fault on the first element of the array (0, 0) on register 'c' on bit 5. The fault is a stuckat and its polarity is high (1)\n",
    "\"\"\"\n",
    "f = flauers.fault_models.StuckAt(x=1, y=1, line=\"c\", bit=5, polarity=1)\n",
    "hw.add_fault(f) # we add the fault "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can perform the matmul with the fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already see that the first element [0, 0] is not correct. This is the effect of the fault.\n",
    "\n",
    "We can double check whether the result is the same with the `==` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C_golden = A_float@B_float\n",
    "C_faulty = hw.matmul(A_float, B_float)\n",
    "C_golden == C_faulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, only the first element is different! Great!\n",
    "\n",
    "Nevertheless, I know it's obvious, but _repetita iuvant_: be careful when comparing with `==` and floating point. Minimal differences affect the result and could result in different outcomes.\n",
    "\n",
    "Prefere comparing the difference like `C_golden-C_faulty < atol` or the relative difference `abs(C_golden-C_faulty)/abs(C_golden) < rtol`\n",
    "\n",
    "> Hint: you can also use np.allclose(C_gloden, C_faulty) to compare the whole tensor at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is possible to clear all the faults and restore the original functionality of the systolic array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw.clear_all_faults()\n",
    "hw.matmul(A_float, B_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C_faulty = hw.matmul(A_float, B_float)\n",
    "np.allclose(C_faulty, C_golden) # As you can see these are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation (especially if we want to use it for NNs) is the convolution. \n",
    "\n",
    "flauers implements a method for convolution that implements the convolution as a combination of some spatial transformation of the input matrices followed by a matrix multiplication. \n",
    "flauers calls implements this strategy to a class called lowlif (which stands for LOwering/LIFting). More details about that are given in the paper [caffe con troll](https://arxiv.org/abs/1504.04343)\n",
    "\n",
    "Nevertheless, for now only one strategy is implemented which has the name of Image to Columns (abbreviated as Im2Col). Let's leave the details aside ðŸ¤­\n",
    "\n",
    "Such method is implemented as a method of the package. Please note that it only supports squared matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "K = np.array( [[1, 1], [1, 1]])\n",
    "C = flauers.convolve(A, K)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify an explicit array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C = flauers.convolve_with_array(A, K, array=hw)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, it is possible to implement the strategy manually using the classes in the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need the input shapes to initialize the im2col object\n",
    "im2col = flauers.lowerings.S_Im2Col( A.shape, K.shape )\n",
    "A_low = im2col.lower_activation( A ) # we can transform the \"activation\"\n",
    "K_low = im2col.lower_kernel( K ) # we can transform the kernel\n",
    "print(\"A is:\\n\", A)\n",
    "print(\"Lowered A is:\\n\", A_low)\n",
    "\n",
    "print(\"K is:\\n\", K)\n",
    "print(\"Lowered K is:\\n\", K_low)\n",
    "\n",
    "C_low = hw.matmul(A_low, K_low)\n",
    "C = im2col.lift(C_low)\n",
    "\n",
    "print(\"Lowered C is:\\n\", C_low)\n",
    "print(\"C is:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Let's see how we can use flauers for PyTorch. First of all, let's load a simple PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=\"valid\")\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=\"valid\")\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# for now map_location=\"cpu\" is mandatory since flauers is not compatible yet with GPU\n",
    "model = torch.load(\"best_model\", map_location=\"cpu\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "This model is trained with LeNet5, so let's load it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "pil_to_tensor = v2.Compose([\n",
    "    # stransforms.RandomResizedCrop(size\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\".\", download=True, train=False, transform=pil_to_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(mnist_test, batch_size = 512, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizing some samples, as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for i in range(9):\n",
    "    img, label = mnist_test[i]\n",
    "\n",
    "    # sugrid visualization (3 by 3)\n",
    "    plt.subplot(3, 3, i+1)\n",
    "\n",
    "    # show the image and label\n",
    "    plt.imshow(img[0], cmap=\"grey\")\n",
    "    plt.title(label)\n",
    "\n",
    "    # Deactivate the tick labels\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "# enhance the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the inference and compute the baseline accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        imgs, labels = batch\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        tot += len(labels)\n",
    "        correct += (outputs.argmax(1) == labels).sum()\n",
    "\n",
    "    acc = correct / tot\n",
    "\n",
    "print(f\"model accuracy is {acc.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We could see that the model accuracy is 99.13% for our model! That's great! \n",
    "\n",
    "But how does it behave when there are faults in the systolic array? Let's try some injections!\n",
    "\n",
    "The first thing we need to do is to tell pytorch to use our hardware when invoking the convolution operation. \n",
    "Luckily for you, flauers offers a ready-to-use wrapper for PyTorch plus some utility functions to allow a smooth transition into the fault injection process.\n",
    "\n",
    "flauers wrappers the hardware in a layer class, called `SystolicConvolution` and available through the `flauers.torch` package and has the same syntax as torch.nn.Conv2d.\n",
    "\n",
    "Nevertheless, we rarely instantiate the layer by ourselves, since usually we have pre-trained models.\n",
    "This means that usually we just want to subsitute a convolutional layer of another model. For this reason there are two usuful functions:\n",
    "\n",
    "- ``flauers.torch.compatible_layers``\n",
    "- ``faluers.torch.replace_layers``\n",
    "\n",
    "Let's see how the process works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flauers.torch\n",
    "\n",
    "print(model)\n",
    "\n",
    "compatible_list = flauers.torch.compatible_layers(model)\n",
    "print(compatible_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function takes the model as an input and spits out a list of `named_submodules` (cfr. torch documentation) that are compatible with flauers.\n",
    "\n",
    "We can then **subsitute** a layer using the second function.\n",
    "Bare in mind, the layer is subsitited in place, so generally we want to instantiate a second copy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flauers.torch.replace_layers(model, compatible_list, hardware=hw)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. Our model is ready for fault injection.\n",
    "\n",
    "To insert the injection we just need to invoke the method `hw.add_fault`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Injection Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to setup a fault injectioncampaign from scratch.\n",
    "\n",
    "First of all, we need two copies of the same model: one will be our golden model, the other will be the faulty model implementing the systolic array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we instantiate the golden model twice\n",
    "model = torch.load(\"best_model\", map_location=\"cpu\")\n",
    "systolic_model = torch.load(\"best_model\", map_location=\"cpu\")\n",
    "\n",
    "# We instantiate the hardware\n",
    "hw = flauers.SystolicArray(\n",
    "    29, 29, 161, # These values are fitting for the later torch experiments\n",
    "    flauers.projection_matrices.output_stationary,\n",
    "    in_dtype = np.dtype(np.float32)\n",
    ")\n",
    "\n",
    "# We then subsitute the layers, to simulate the systolic array doing its thing\n",
    "compatible_layers = flauers.torch.compatible_layers(model)\n",
    "flauers.torch.replace_layers(systolic_model, compatible_layers, hardware=hw)\n",
    "print(model)\n",
    "print(systolic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try an inference on the systolic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    imgs, labels = batch\n",
    "    output = systolic_model(imgs)\n",
    "    correct += (outputs.argmax(1) == labels).sum()\n",
    "    print(f\"Systolic batch accuracy is {correct / output.shape[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, there is a progressbar (implemented with tqdm) that shows the progress of the processing per batch per systolic array.\n",
    "This means that for our model the progress bar will show up twice (becuase there are two systolic convolutions).\n",
    "\n",
    "> Note: It is not possible to use flauers with the gradient computation (since the backward pass is not implemented), so please don't forget the statement `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this example we implemented some classes for ease of use of the tool.\n",
    "\n",
    "In general, this is not needed but they highly simplify the injection process to almost completely automating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Generator\n",
    "\n",
    "FaultGenerator is a class that automatically generates the parameters of a fault, given the space parameters. Also, it implements hotloading from a state dictionary that is written on the disk invoking the method `save_state`.\n",
    "The method `get_fault` automatically generates a StuckAt fault in a random PE.\n",
    "Finally, the [Leveugle formula from Date 2009 on statistical fault injection](https://ieeexplore.ieee.org/document/5090716) is directly implemented inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class FaultGenerator:\n",
    "    def __init__(self, hw: flauers.SystolicArray,\n",
    "                 name: None|str = None, automatic_save: bool = False, save_path: str = './_fidata', \n",
    "                 channels: int = 6, bits: int = 32, try_continue: bool = False\n",
    "                ):\n",
    "        self.hw = hw\n",
    "        self.next_seed = 0\n",
    "        self.channels = channels\n",
    "        self.bits = bits\n",
    "        self.possible_elements = list( self.hw.physical_mapping.keys() )\n",
    "\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        if automatic_save and name is None:\n",
    "                raise Exception(\"You must give it a name to save the file\")\n",
    "        self.save_path = save_path\n",
    "        self.automatic_save = automatic_save\n",
    "\n",
    "        self._next_minus_one = True\n",
    "        \n",
    "        if try_continue:\n",
    "            if name is None:\n",
    "                raise Exception(\"You must give it a name to hot-reload\")\n",
    "            fn = os.path.join( save_path, f\"FaultGenerator_{name}_state.pkl\" )\n",
    "            if not os.path.exists(fn):\n",
    "                print(f\"No old state file {fn} found. Starting from zero.\")\n",
    "            else:\n",
    "                # print(f\"Reloading state from {fn}.\")\n",
    "                self.load_state()\n",
    "\n",
    "    def current_seed(self):\n",
    "        return self.next_seed - 1\n",
    "    \n",
    "    def starting_point(self):\n",
    "        return self.next_seed\n",
    "    \n",
    "    def statistical_faults(self,\n",
    "                       error_margin: float = 0.05, \n",
    "                       confidence_level: float = 1.96,\n",
    "                       error_probability: float = 0.5):\n",
    "        # Population parameters\n",
    "        n_channels = self.channels\n",
    "        n_PEs = len(self.possible_elements)\n",
    "        bits = self.bits\n",
    "        \n",
    "        lines = 3 # a, b, c - always\n",
    "        polarities = 2 # 0 or 1 - always (with stuck-ats at least)\n",
    "\n",
    "        # Total population\n",
    "        N = n_channels * n_PEs * lines * bits * polarities\n",
    "\n",
    "        # Statistical parameters\n",
    "        e = error_margin\n",
    "        t = confidence_level\n",
    "        p = error_probability\n",
    "\n",
    "        # Date 2009 Formula\n",
    "        n = N / ( 1 + e**2 *  (N-1)/(t**2 * p * (1-p) ) )\n",
    "\n",
    "        print(f\"N is {N}. We need to inject {n} faults\", end= \" \")\n",
    "        n_faults = int(np.ceil(n))\n",
    "        print(f\"which actually are {n_faults}\")\n",
    "\n",
    "        return n_faults\n",
    "\n",
    "    def get_fault(self):\n",
    "        random.seed(self.next_seed)\n",
    "        line = random.choice([\"a\", \"b\", \"c\"])\n",
    "        channel = random.randrange(0, self.channels)\n",
    "        x, y = random.choice( self.possible_elements )\n",
    "        bit = random.randrange(0, self.bits)\n",
    "        polarity = random.randrange(0, 2)\n",
    "        \n",
    "        f = flauers.fault_models.StuckAt(\n",
    "            line, \n",
    "            x = x, y = y,\n",
    "            bit = bit, polarity = polarity\n",
    "        )\n",
    "\n",
    "        self._next_minus_one = True\n",
    "        \n",
    "        self.next_seed += 1\n",
    "        if self.automatic_save:\n",
    "            self.save_state()\n",
    "            \n",
    "        return (f, channel)\n",
    "\n",
    "    def iteration_done(self):\n",
    "        self._next_minus_one = False\n",
    "        if self.automatic_save:\n",
    "            self.save_state()\n",
    "\n",
    "    def load_state(self, file_name: None|str = None, file_path: None|str = None):\n",
    "        \"\"\" Check save_state to understand the parameters \"\"\"\n",
    "        if file_name is not None and not os.path.exists(file_path):\n",
    "            raise Exception(f\"Directory {file_path} does not exist.\")\n",
    "\n",
    "        if file_path is not None: path = file_path\n",
    "        else: path = self.save_path\n",
    "\n",
    "        if file_name is not None: path = os.path.join(path, f\"{file_name}\")\n",
    "        elif self.name is not None: path = os.path.join(path, f\"FaultGenerator_{self.name}_state.pkl\")\n",
    "        else: raise Exception(f\"A name must be given!.\")\n",
    "\n",
    "        print(f\"Reloading FaultGenerator {self.name if self.name is not None else ''} from file {path} ...\", end=\" \")\n",
    "            \n",
    "        file = open(path, \"rb\")\n",
    "        state_dict = pickle.load(file)\n",
    "\n",
    "        if \"_next_minus_one\" in state_dict:\n",
    "            self._next_minus_one = state_dict[\"_next_minus_one\"]\n",
    "        else:\n",
    "            self._next_minus_one = False\n",
    "        \n",
    "        self.next_seed = state_dict[\"next_seed\"]\n",
    "        if self._next_minus_one:\n",
    "         self.next_seed -= 1\n",
    "\n",
    "        print(f\"Next seed will be {self.next_seed}\")\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    def save_state(self, file_name:None|str = None, file_path:None|str = None):\n",
    "        state_dict = {\n",
    "            \"next_seed\": self.next_seed,\n",
    "            \"_next_minus_one\": self._next_minus_one\n",
    "        }\n",
    "        \n",
    "        # The path is either the given or the default\n",
    "        path = self.save_path if file_path is None else file_path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # The name is the given parameter\n",
    "        if file_name is not None: \n",
    "            path = os.path.join(path, f\"FaultGenerator_{file_name}_state.pkl\")\n",
    "        # or the name given in the initializer\n",
    "        elif self.name is not None:\n",
    "            path = os.path.join(self.save_path, f\"FaultGenerator_{self.name}_state.pkl\")\n",
    "        # if no name is provided either ways, there's an error.\n",
    "        else:\n",
    "            raise Exception(\"A name is necessary!\")\n",
    "            \n",
    "        file = open(path, \"wb\")\n",
    "        pickle.dump(state_dict, file)\n",
    "        file.flush()\n",
    "        file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Loader\n",
    "\n",
    "BatchLoader is a class that implements the automatic shuffling of the dataset with a fixed size. This is needed simply for the fact that, due to simulation timings, it is almost impossible to run the systolic array on the whole dataset. So the strategy in this case is to load a fixed number (`batch_size` to be precise) of images in random order for every injected fault.\n",
    "So each batch will correspond to a different injected fault.\n",
    "\n",
    "Please note that also this class has the option for saving and loading the state in case the experiments are abruptly interrupted for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, dataset, #: torchvision.datasets.VisionDataset,\n",
    "                 name: None|str = None, automatic_save: bool = False, save_path: str = './_fidata',  # fault injection data\n",
    "                 batch_size = 64, try_continue: bool = False\n",
    "                ):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.hw = hw\n",
    "        self.len_dataset = len(dataset)\n",
    "        \n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "            \n",
    "        self.automatic_save = automatic_save\n",
    "        if automatic_save and name is None:\n",
    "                raise Exception(\"You must give it a name to save the file\")\n",
    "        self.save_path = save_path\n",
    "\n",
    "        if self.len_dataset % batch_size  != 0:\n",
    "            raise Exception(f\"\"\"Please, select a batch-size multiple of the length of the dataset. \n",
    "            You selected {batch_size} The dataset is composed by {self.len_dataset} elements. \n",
    "            {self.len_dataset} is not divisible by {batch_size}\"\"\")\n",
    "\n",
    "        self.seed = 0\n",
    "        self.next_index = 0\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._next_minus_one = True\n",
    "        \n",
    "        random.seed(self.seed)\n",
    "        self._order = [i for i in range( self.len_dataset )]\n",
    "        random.shuffle(self._order)\n",
    "\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "        if try_continue:\n",
    "            if name is None:\n",
    "                raise Exception(\"You must give it a name to hot-reload\")\n",
    "            fn = os.path.join( save_path, f\"BatchLoader_{self.name}_state.pkl\" )\n",
    "            if not os.path.exists(fn):\n",
    "                print(f\"No old state file {fn} found. Starting from zero.\")\n",
    "            else:\n",
    "                # print(f\"Reloading state from {fn}.\")\n",
    "                self.load_state()\n",
    "\n",
    "    def current_index(self):\n",
    "        if self._next_minus_one:\n",
    "            return self.next_index - 1\n",
    "        return self.next_index\n",
    "        \n",
    "    def current_seed(self):\n",
    "        if self.next_index == 0:\n",
    "            return self.seed - 1\n",
    "        return self.seed\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        self._order = [i for i in range( self.len_dataset )]\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(self._order)\n",
    "    \n",
    "    def load_state(self, file_name: None|str = None, file_path: None|str = None):\n",
    "        \"\"\" Check save_state to understand the parameters \"\"\"\n",
    "        if file_name is not None and not os.path.exists(file_path):\n",
    "            raise Exception(f\"Directory {file_path} does not exist.\")\n",
    "\n",
    "        if file_path is not None: path = file_path\n",
    "        else: path = self.save_path\n",
    "\n",
    "        if file_name is not None: path = os.path.join(path, f\"{file_name}\")\n",
    "        elif self.name is not None: path = os.path.join(path, f\"BatchLoader_{self.name}_state.pkl\")\n",
    "        else: raise Exception(f\"A name must be given!.\")\n",
    "\n",
    "        print(f\"Reloading BatchLoader {self.name if self.name is not None else ''} from file {path}...\", end=\" \" )\n",
    "        \n",
    "        file = open(path, \"rb\")\n",
    "        state_dict = pickle.load(file)\n",
    "        self.seed = state_dict[\"seed\"]\n",
    "        if \"_next_minus_one\" in state_dict:\n",
    "            self._next_minus_one = state_dict[\"_next_minus_one\"]\n",
    "        else:\n",
    "            self._next_minus_one = False\n",
    "        self.next_index = state_dict[\"next_index\"]\n",
    "        if self._next_minus_one: \n",
    "            self.next_index -= 1\n",
    "        if self.next_index < 0:\n",
    "            self.next_index = 0\n",
    "            self.seed -= 1\n",
    "        self.batch_size = state_dict[\"batch_size\"]\n",
    "        self.len_dataset = state_dict[\"len_dataset\"]\n",
    "\n",
    "        print(f\"Next index will be {self.next_index} with seed {self.seed}\")\n",
    "        \n",
    "        file.close()\n",
    "\n",
    "    def save_state(self, file_name: None|str = None, file_path: None|str = None):\n",
    "        \"\"\"\n",
    "            Save the state for hot-reloading.\n",
    "            If the object is initialized with a name, that will be used to save the state.\n",
    "            Otherwise, file_name is mandatory to save the file. file_path is optional. If not given,\n",
    "            the default ./_fidata will be used.\n",
    "            \n",
    "            Parameters\n",
    "            ---\n",
    "            file_name: [Optional] the name given to the state_file. It is mandatory if the object is initialized without a name\n",
    "            file_path: [Optional] the path in which to save the file. If not given the default ./_fidata is used\n",
    "        \"\"\"\n",
    "        state_dict = {\n",
    "            \"seed\": self.seed,\n",
    "            \"next_index\": self.next_index,\n",
    "            \"batch_size\": self._batch_size,\n",
    "            \"len_dataset\": self.len_dataset,\n",
    "            \"_next_minus_one\": self._next_minus_one,\n",
    "        }\n",
    "\n",
    "        # The path is either the given or the default\n",
    "        path = self.save_path if file_path is None else file_path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # The name is the given parameter\n",
    "        if file_name is not None: \n",
    "            path = os.path.join(path, f\"BatchLoader_{file_name}_state.pkl\")\n",
    "        # or the name given in the initializer\n",
    "        elif self.name is not None:\n",
    "            path = os.path.join(self.save_path, f\"BatchLoader_{self.name}_state.pkl\")\n",
    "        # if no name is provided either ways, there's an error.\n",
    "        else:\n",
    "            raise Exception(\"A name is necessary!\")\n",
    "            \n",
    "        file = open(path, \"wb\")\n",
    "        pickle.dump(state_dict, file)\n",
    "        file.flush()\n",
    "        file.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        img_shape = self.dataset[0][0].shape\n",
    "        tot_batches = self.len_dataset / self._batch_size\n",
    "        if self.next_index >= tot_batches: # let's make a new order\n",
    "            self.seed += 1\n",
    "            random.seed(self.seed)\n",
    "            self._order = [i for i in range(self.len_dataset)]\n",
    "            random.shuffle(self._order)\n",
    "            self.next_index = 0\n",
    "\n",
    "        starting_point = self.next_index * self._batch_size \n",
    "        imgs = np.zeros( (self._batch_size, *img_shape), dtype=np.float32)\n",
    "        labels = np.zeros( self._batch_size, dtype=np.float32)\n",
    "        for i in range(self._batch_size):\n",
    "            idx = self._order[ i + starting_point ]\n",
    "            img, label = self.dataset[ idx ]\n",
    "            imgs[i, :, :] = img\n",
    "            labels[i] = label\n",
    "        self._next_minus_one = True\n",
    "        self.next_index += 1\n",
    "        if self.automatic_save:\n",
    "            self.save_state()\n",
    "        return ( torch.from_numpy(imgs), torch.from_numpy(labels) )\n",
    "\n",
    "    def iteration_done(self):\n",
    "        self._next_minus_one = False\n",
    "        if self.automatic_save:\n",
    "            self.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Writer\n",
    "\n",
    "ResultWriter implements the dumping of the results on a csv file. Ultimately, this is the result of the fault injection campaign. It contains the data gathered through the fault injection process. Also in this case, there is the option of saving and loading the state, to recover the experiments in case of abrupt stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, csv, pickle\n",
    "\n",
    "class ResultsWriter():\n",
    "    def __init__(self, file_path, \n",
    "                 try_continue = True,\n",
    "                 fields = [ \"fault_seed\", \"dataset_seed\", \"dataset_index\",\n",
    "                          \"line\", \"channel\", \"row\", \"col\", \"bit\", \"polarity\", \"ftype\"],\n",
    "                ):\n",
    "        self.file_path = file_path\n",
    "        self.fields = fields\n",
    "\n",
    "        newFile = False # This is needed to write the header\n",
    "        \n",
    "        # Open the file\n",
    "        # There are 4 cases:\n",
    "        #     1. try_continue = False and file exists -> raise exception.\n",
    "        #     2. try_continue = True and file exists -> then we read the existing file\n",
    "        #     3. try_continue = False and file not exists -> we make a new file\n",
    "        #     4. try_continue = True and file not exists -> we create the file\n",
    "        if os.path.exists(file_path) and not try_continue: # case 1.\n",
    "            raise Exception(f\"File {file_path} exists, but hot-reloading is disabled.\")\n",
    "        if try_continue and os.path.exists(file_path): # case 2.\n",
    "            if not self._validate_csv(file_path): raise Exception(f\"Fields are different!\")\n",
    "            self.file = open(file_path, \"a\")\n",
    "        else: # case 3. and 4.\n",
    "            parent = os.path.dirname(file_path)\n",
    "            if not os.path.exists(parent):\n",
    "                os.makedirs(parent)\n",
    "            self.file = open(file_path, \"w\")\n",
    "            newFile = True\n",
    "            \n",
    "        self.writer = csv.DictWriter(self.file, fieldnames = fields)\n",
    "        if newFile:\n",
    "            print(f\"Initializing file {file_path}...\", end =\"\")\n",
    "            self.writer.writeheader()\n",
    "            self.file.flush()\n",
    "            print(f\" Header wrote.\")\n",
    "\n",
    "    def write_row(self, data: list|dict, multiple_rows: bool = False):\n",
    "        if multiple_rows:\n",
    "            self.writer.writerows(data)\n",
    "        else:\n",
    "            self.writer.writerow(data)\n",
    "        self.file.flush()\n",
    "\n",
    "    def close():\n",
    "        self.file.close()\n",
    "    \n",
    "    def _validate_csv(self, file_path):\n",
    "        file = open(file_path, \"r\")\n",
    "        csv_file = csv.DictReader(file)\n",
    "        try:\n",
    "            row = next(iter(csv_file))\n",
    "        except StopIteration: # The file exists but it's empty\n",
    "            return True\n",
    "            \n",
    "        read_fields = row.keys()\n",
    "        file.close()\n",
    "\n",
    "        for field in self.fields:\n",
    "            if field not in read_fields:\n",
    "                print(f\"field {field} does not exist in the file.\")\n",
    "                return False\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Injection Campaign\n",
    "\n",
    "Finally we reach the core of the fault injection. This class runs a fault injection campaign using the classes previously defined and computes the metrics. \n",
    "This class only implements the \"fault_type\" class but can be easily extended to compute other metrics like SDC1 or SDC5.\n",
    "\n",
    "Nevertheless, in theory it is possible to retrieve the metrics by saving the complete output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "fault_types_names = [\"masked\", \"non-critical\", \"sdc1\"]\n",
    "\n",
    "class FaultInjectionCampaign:\n",
    "    def __init__(self, gmodel, fmodel, dataset, hw, channels, bits,\n",
    "                model_name: str, dataset_name: str, hw_name: str,\n",
    "                faulty_layers: list, bs_size = 100):\n",
    "        \"\"\"\n",
    "        THE LAYER SUBSTITUTION SHOULD BE ALREADY DONE!\n",
    "        \"\"\"\n",
    "        self.golden_model = gmodel\n",
    "        self.faulty_model = fmodel\n",
    "        self.dataset = dataset\n",
    "        self.hw = hw\n",
    "\n",
    "        self.faulty_layers = faulty_layers\n",
    "\n",
    "        name = f\"{model_name}.{dataset_name}.{hw_name}\"\n",
    "        self.name = name\n",
    "        self.batch_loader = BatchLoader( dataset, name=name, batch_size = bs_size,\n",
    "                                        automatic_save=True, try_continue=True )\n",
    "        self.fault_generator = FaultGenerator(hw, channels = channels, bits = bits,\n",
    "                                              name=name, automatic_save=True, try_continue=True )\n",
    "        self.results_writer = ResultsWriter( f\"./results/{name}.csv\", try_continue=True )\n",
    "\n",
    "        self.golden_model.eval()\n",
    "        self.faulty_model.eval()\n",
    "\n",
    "    def run_golden_inference(self):\n",
    "        return self.golden_model(self.dataset)\n",
    "\n",
    "    def compute_fault_type(self, golden, faulty: np.ndarray):\n",
    "        gout = golden\n",
    "        fout = faulty\n",
    "        \n",
    "        bs = gout.shape[0]\n",
    "        gmax, glabel = torch.max(gout, 1)\n",
    "        fmax, flabel = torch.max(fout, 1)\n",
    "        logits = np.array( torch.allclose(gout, fout, rtol=0.01) )\n",
    "    \n",
    "        # default is sdc1\n",
    "        fault_types = 2 * np.ones((bs))\n",
    "        # if the labels are the same, the fault is non-critical    \n",
    "        fault_types[ glabel == flabel ] = 1\n",
    "        # if the logits are the same, masked\n",
    "        fault_types[ np.all(logits, axis=tuple([i for i in range(1, len(logits.shape))]) ) ] = 0\n",
    "    \n",
    "        return fault_types\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        n_faults = self.fault_generator.statistical_faults()\n",
    "        start = self.fault_generator.starting_point()\n",
    "        \n",
    "        for i in trange(start, n_faults, dynamic_ncols=True, leave=True):\n",
    "            # batch and fault generation\n",
    "            batch = self.batch_loader.get_batch()\n",
    "            fault, channel = self.fault_generator.get_fault()\n",
    "\n",
    "            # fault instantiation\n",
    "            for layer in self.faulty_layers:\n",
    "                layer.clear_faults()\n",
    "                layer.add_fault(fault, channel)\n",
    "\n",
    "            # compute the inferences\n",
    "            img, labels = batch\n",
    "            with torch.no_grad():\n",
    "                golden_output = self.golden_model(img)\n",
    "                faulty_output = self.faulty_model(img)\n",
    "    \n",
    "            ftype = self.compute_fault_type(golden_output, faulty_output)\n",
    "\n",
    "            res = []\n",
    "            for i in range( len(ftype) ):\n",
    "                res.append( {\n",
    "                    \"fault_seed\": self.fault_generator.current_seed(), \n",
    "                    \"dataset_seed\": self.batch_loader.current_seed(),\n",
    "                    \"dataset_index\": self.batch_loader.current_index(),\n",
    "                    \"line\": fault.line.name,\n",
    "                    \"channel\": channel,\n",
    "                    \"row\": fault.x, \n",
    "                    \"col\": fault.y, # respectively row and column\n",
    "                    \"bit\": fault.bit,\n",
    "                    \"polarity\": fault.polarity,\n",
    "                    \"ftype\": int(ftype[i]),\n",
    "                } )\n",
    "\n",
    "            self.results_writer.write_row(res, multiple_rows=True)\n",
    "            self.batch_loader.iteration_done()\n",
    "            self.fault_generator.iteration_done()\n",
    "\n",
    "    def done(self):\n",
    "        f = open(f\"tmp.done.{self.name}\", \"w\")\n",
    "        f.write(\"DONE!\\n\")\n",
    "        f.write(f\"{datetime.now()}\")\n",
    "        f.flush()\n",
    "        f.close()\n",
    "        print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actual Fault injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the helper classes defined before, we can simply call the run method of the FaultInjectionCampaign to run the whole thing.\n",
    "Because we implemented the thing using tqdm, we have an estimation of the needed time for a fault injection campaign. \n",
    "\n",
    "> *Bonus*: since we implemented a mechanism for saving the state and reloading the state, it is possible to interrupt the process anytime and will resume from the last fault recorded! That's great in case of a crash of the system. Also, it is possible to disable the _hot-reloading_ by passing `False` to the `try_continue` and `automatic_save` parameters of the various objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faulty_layers = [\n",
    "    systolic_model.get_submodule(\"conv1\"),\n",
    "    systolic_model.get_submodule(\"conv2\"),\n",
    "]\n",
    "fault_injection_campaign = FaultInjectionCampaign(\n",
    "    model, systolic_model, mnist_test, hw, 6, 32, \n",
    "                                \"lenet\", \"mnist\", \"os\", faulty_layers, bs_size = 100\n",
    ")\n",
    "fault_injection_campaign.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saffira_env",
   "language": "python",
   "name": "saffira_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
